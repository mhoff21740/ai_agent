import os
import argparse
from dotenv import load_dotenv
import google.generativeai as genai
from google.generativeai import types
from functions.get_files_info import get_files_info, schema_get_files_info
from functions.get_file_content import get_file_content, schema_get_file_content
from functions.write_file import write_file, schema_write_file
from functions.run_python import run_python_file, schema_run_python
from typing import Any

# Map function names (as used by the model) to actual Python functions
FUNCTION_MAP = {
    "get_files_info": get_files_info,
    "get_file_content": get_file_content,
    "write_file": write_file,
    "run_python": run_python_file,
}

# Declare available tools (schemas for Gemini to understand the functions it can call)
AVAILABLE_FUNCTIONS = types.Tool(
    function_declarations=[
        schema_get_files_info,
        schema_get_file_content,
        schema_write_file,
        schema_run_python,
    ],
)

# Instruction for how the model should behave and what it's allowed to do
SYSTEM_PROMPT = """
You are a helpful AI coding agent.

When a user asks a question or makes a request, make a function call plan. You can perform the following operations:

- List files and directories
- Read file contents
- Execute Python files with optional arguments
- Write or overwrite files

All paths you provide should be relative to the working directory. You do not need to specify the working directory in your function calls as it is automatically injected for security reasons.
"""

# Extract function calls from the model's response
def extract_function_calls(resp: genai.types.GenerateContentResponse):
    calls = []
    for cand in resp.candidates or []:
        for part in cand.content.parts:
            call = getattr(part, "function_call", None)
            if call:
                calls.append(call)
    return calls

# Call a specific function and return the result in Gemini-compatible format
def call_function(call: Any, verbose=False) -> dict:
    fn_name = call.name
    fn_args = dict(call.args or {})
    fn_args["working_directory"] = "./calculator"

    # Normalize field if directory is misused for file-based functions
    if fn_name != "get_files_info" and "directory" in fn_args:
        fn_args["file_path"] = fn_args.pop("directory")

    if verbose:
        print(f"Calling function: {fn_name}({fn_args})")
    else:
        print(f" - Calling function: {fn_name}")

    fn = FUNCTION_MAP.get(fn_name)
    if not fn:
        payload = {"error": f"Unknown function: {fn_name}"}
    else:
        try:
            payload = {"result": fn(**fn_args)}
        except Exception as exc:
            payload = {"error": str(exc)}

    return {
        "role": "tool",
        "parts": [
            {
                "function_response": {
                    "name": fn_name,
                    "response": payload,
                }
            }
        ],
    }

# Main function orchestrates the entire workflow
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("prompt", help="Initial user prompt")
    ap.add_argument("--verbose", action="store_true")
    args = ap.parse_args()

    load_dotenv()  # Load environment variables from .env file
    api_key = os.getenv("GOOGLE_API_KEY") or ""
    if not api_key:
        raise RuntimeError("GOOGLE_API_KEY missing")
    genai.configure(api_key=api_key)  # Initialize the Gemini client

    # Instantiate the model
    model = genai.GenerativeModel(
        model_name="gemini-2.0-flash-001",
        system_instruction=SYSTEM_PROMPT,
    )

    messages = [{"role": "user", "parts": [args.prompt]}]  # Conversation history

    for step in range(20):  # Limit to 20 steps to prevent infinite loops
        try:
            resp = model.generate_content(
                contents=messages,
                tools=[AVAILABLE_FUNCTIONS],
            )
        except Exception as e:
            print(f"Fatal API error: {e}")
            return

        # Append model output to messages
        for cand in resp.candidates or []:
            messages.append({"role": "model", "parts": cand.content.parts})

        # Try to extract and print any text-based response
        plain_text = None
        try:
            first_parts = resp.candidates[0].content.parts
            text_chunks = [p.text for p in first_parts if getattr(p, "text", None)]
            plain_text = " ".join(text_chunks).strip() if text_chunks else None
        except Exception:
            plain_text = None

        if plain_text:
            print("\nModel response:\n" + plain_text)
            return

        # If no plain text, check for function calls
        calls = extract_function_calls(resp)
        if not calls:
            print("No function call and no text â€“ stopping.")
            return

        for call in calls:
            tool_msg = call_function(call, verbose=args.verbose)
            try:
                _ = tool_msg["parts"][0]["function_response"]["response"]
            except Exception:
                raise RuntimeError("Invalid tool response structure")

            messages.append(tool_msg)  # Add function result to message history
            if args.verbose:
                print(f"-> {_}")

    print("Reached 20 iterations without a final answer.")

if __name__ == "__main__":
    main()
